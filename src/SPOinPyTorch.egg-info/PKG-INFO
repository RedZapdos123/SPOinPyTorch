Metadata-Version: 2.4
Name: SPOinPyTorch
Version: 0.1.0
Summary: Simple Policy Optimization (SPO) algorithm implementation in PyTorch for reinforcement learning
Author: SPOinPyTorch Contributors
Maintainer: SPOinPyTorch Contributors
License: MIT
Project-URL: Homepage, https://github.com/SPOinPyTorch/SPOinPyTorch
Project-URL: Documentation, https://github.com/SPOinPyTorch/SPOinPyTorch#readme
Project-URL: Repository, https://github.com/SPOinPyTorch/SPOinPyTorch.git
Project-URL: Bug Tracker, https://github.com/SPOinPyTorch/SPOinPyTorch/issues
Keywords: reinforcement-learning,policy-optimization,pytorch,spo,continuous-control,machine-learning,deep-learning
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch>=2.0.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: gymnasium>=0.28.0
Provides-Extra: examples
Requires-Dist: matplotlib>=3.5.0; extra == "examples"
Requires-Dist: seaborn>=0.11.0; extra == "examples"
Requires-Dist: optuna>=3.0.0; extra == "examples"
Requires-Dist: tqdm>=4.60.0; extra == "examples"
Requires-Dist: box2d-py>=2.3.5; extra == "examples"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: isort>=5.10.0; extra == "dev"
Requires-Dist: flake8>=5.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Provides-Extra: all
Requires-Dist: SPOinPyTorch[dev,examples]; extra == "all"
Dynamic: license-file

# SPOinPyTorch

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A PyTorch implementation of the **Simple Policy Optimization (SPO)** algorithm for reinforcement learning in continuous control environments.

SPO is a recent alternative to Proximal Policy Optimization (PPO) that offers improved sample efficiency and stability through a simplified objective function with quadratic penalty terms.

## üöÄ Features

- **Clean, modular implementation** of the SPO algorithm
- **Easy-to-use API** similar to popular RL libraries
- **Optimized for continuous control** tasks
- **Cross-platform compatibility** (Linux, Windows, macOS)
- **GPU acceleration** support
- **Comprehensive examples** including LunarLander training
- **Hyperparameter optimization** with Optuna integration
- **Visualization tools** for training analysis

## üì¶ Installation

### From PyPI (Recommended)

```bash
pip install SPOinPyTorch
```

### From Source

```bash
git clone https://github.com/SPOinPyTorch/SPOinPyTorch.git
cd SPOinPyTorch
pip install -e .
```

### With Examples Dependencies

```bash
pip install SPOinPyTorch[examples]
```

## üéØ Quick Start

### Basic Usage

```python
import torch
import gymnasium as gym
from SPOinPyTorch import SPOAgent, Config

# Create environment
env = gym.make("LunarLanderContinuous-v3")

# Initialize configuration
config = Config()

# Create SPO agent
agent = SPOAgent(
    state_dim=8,
    action_dim=2,
    config=config.get_dict(),
    is_discrete=False,
    action_low=[-1, -1],
    action_high=[1, 1],
    device='cuda' if torch.cuda.is_available() else 'cpu'
)

# Training loop (simplified)
state, _ = env.reset()
for step in range(1000):
    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
    action, log_prob, entropy, value = agent.get_action_and_value(state_tensor)
    
    next_state, reward, terminated, truncated, _ = env.step(action.cpu().numpy()[0])
    
    # Update agent (requires batch of experiences)
    # See examples for complete training implementation
    
    state = next_state
    if terminated or truncated:
        state, _ = env.reset()
```

### Training on LunarLander

```python
# See examples/lunar_lander/train.py for complete implementation
from SPOinPyTorch import SPOAgent, Config

config = Config()
# ... training loop implementation
```

## üìö Examples

The `examples/` directory contains complete implementations:

- **`lunar_lander/train.py`** - Full training script
- **`lunar_lander/evaluate.py`** - Model evaluation and visualization
- **`lunar_lander/hyperparameter_optimization.py`** - Optuna-based hyperparameter tuning
- **`lunar_lander/visualization.py`** - Training progress visualization

### Running Examples

```bash
cd examples/lunar_lander

# Train a new agent
python train.py

# Evaluate trained agent
python evaluate.py

# Optimize hyperparameters
python hyperparameter_optimization.py

# Visualize training progress
python visualization.py
```

## üèÜ Performance

SPO demonstrates superior performance compared to PPO on LunarLanderContinuous-v3:

| Metric | SPO | PPO |
|--------|-----|-----|
| Average Reward | **229.37 ¬± 78.15** | 220.2 ¬± 74.6 |
| Success Rate (‚â•200) | **83%** | 80% |
| Training Stability | **High** | Moderate |
| Sample Efficiency | **Improved** | Baseline |

## üîß API Reference

### SPOAgent

The main agent class implementing the SPO algorithm.

```python
SPOAgent(
    state_dim: int,           # Dimension of state space
    action_dim: int,          # Dimension of action space  
    config: dict,             # Configuration dictionary
    is_discrete: bool = True, # Whether action space is discrete
    action_low: list = None,  # Lower bounds for continuous actions
    action_high: list = None, # Upper bounds for continuous actions
    device: str = 'cpu'       # Device to run on ('cpu' or 'cuda')
)
```

**Key Methods:**
- `get_action_and_value(state)` - Get action, log probability, entropy, and value
- `get_value(state)` - Get state value estimate
- `compute_gae(rewards, dones, values, next_value)` - Compute GAE advantages
- `update(states, actions, old_log_probs, advantages, returns)` - Update policy

### Config

Configuration class with optimized hyperparameters.

```python
config = Config()
config.learning_rate = 0.0003  # Modify parameters
config_dict = config.get_dict()  # Get as dictionary
```

### Models

Neural network architectures for actor and critic.

```python
from SPOinPyTorch import Actor, Critic

actor = Actor(state_dim=8, action_dim=2, hidden_dims=[256, 256], is_discrete=False)
critic = Critic(state_dim=8, hidden_dims=[256, 256])
```

## üî¨ Algorithm Details

SPO modifies the PPO objective with a quadratic penalty term:

```
L_SPO = r(Œ∏) * A - (|A| / 2Œµ) * (r(Œ∏) - 1)¬≤
```

Where:
- `r(Œ∏)` is the probability ratio
- `A` is the advantage estimate  
- `Œµ` is the clipping parameter

This formulation provides:
- **Better sample efficiency** through improved gradient estimates
- **Enhanced stability** via quadratic penalty regularization
- **Simplified implementation** compared to PPO's clipped objective

## üõ†Ô∏è Development

### Setting up Development Environment

```bash
git clone https://github.com/SPOinPyTorch/SPOinPyTorch.git
cd SPOinPyTorch

# Install in development mode
pip install -e .[dev]

# Run tests
pytest

# Format code
black src/ examples/
isort src/ examples/

# Type checking
mypy src/
```

### Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üìñ Citation

If you use SPOinPyTorch in your research, please cite:

```bibtex
@article{spo2024,
  title={Simple Policy Optimization},
  author={SPO Authors},
  journal={arXiv preprint arXiv:2401.16025},
  year={2024}
}

@software{spoinpytorch2024,
  title={SPOinPyTorch: Simple Policy Optimization in PyTorch},
  author={SPOinPyTorch Contributors},
  year={2024},
  url={https://github.com/SPOinPyTorch/SPOinPyTorch}
}
```

## üîó References

- [Simple Policy Optimization Paper](https://arxiv.org/abs/2401.16025)
- [PyTorch](https://pytorch.org/)
- [Gymnasium](https://gymnasium.farama.org/)
- [Optuna](https://optuna.org/)

## ü§ù Acknowledgments

- Original SPO research paper authors
- PyTorch and Gymnasium communities
- Contributors and users of this library

---

**Made with ‚ù§Ô∏è for the reinforcement learning community**
